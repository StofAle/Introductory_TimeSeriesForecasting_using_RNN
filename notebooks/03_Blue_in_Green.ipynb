{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "#### In this notebook, the optimal network configurations obtained from the runs in the preceding notebook ('02_Freddie_Freeloader.ipynb') are applied to the clean data set with added random noise. The noise is added in two different ways: \n",
    "#### 1) 'Add distortion': The noise is fed as separate input to the model while the target variable remains the clean time series\n",
    "#### 2) 'Distort signal': The noise is added ontop of the clean time series data, which is taken as the target variable.\n",
    "#### In both scenarios, the model performance is measures for varying standard deviation of the noise level.\n",
    "\n",
    "#### The best performing model configuration will be used in the notebook '03_Blue_in_Green.ipynb' to analyze the impact of adding noise to the clean dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [1. Load modules](#Part1_link)\n",
    "* [2. Distortion next to clean time series](#Part2_link)\n",
    "<br >&nbsp;&nbsp;&nbsp;[2.1 Evaluate model performance under varying noise levels](#Part2.1_link)\n",
    "<br >&nbsp;&nbsp;&nbsp;[2.2 Visualize and save results](#Part2.2_link)\n",
    "* [3. Distorted time series](#Part3_link)\n",
    "<br >&nbsp;&nbsp;&nbsp;[3.1 Evaluate model performance under varying noise levels](#Part3.1_link)\n",
    "<br >&nbsp;&nbsp;&nbsp;[3.2 Visualize and save results](#Part3.2_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part1_link'></a>\n",
    "# 1. Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import Kind_of_Blue  # own class with a collection of methods used in this analysis\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part2_link'></a>\n",
    "# 2. Distortion next to clean time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run RNN and LSTM model with noise as separate feature. Evaluate model performance for a range of variances of the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a range of dates on which the observations are made\n",
    "idx = pd.date_range(end='7/1/2020', periods=5*364, freq='d')\n",
    "\n",
    "# take a sine function as the observations\n",
    "num_periods = 10  # number of sine periods\n",
    "observations = [np.sin(2*np.pi*num_periods*x/len(idx)) for x in range(len(idx))]\n",
    "\n",
    "# initialize object\n",
    "mdq = Kind_of_Blue.Kind_of_Blue()\n",
    "\n",
    "# set target feature \n",
    "mdq._selected_features = ['observations']\n",
    "\n",
    "# set number of time points for 1/ future forecasting points and 2/ the past, historical time points\n",
    "future_target_size = int(365/52)\n",
    "past_history_size = int(1*365)\n",
    "\n",
    "# specify model configuration: this is chosen basen on the results from the previous notebook 02_Freddie_Freeloader.ipynb\n",
    "units = 256  # number of units in each neural network layer\n",
    "num_layers = 2  # total number of layers\n",
    "epochs = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part2.1_link'></a>\n",
    "### 2.1 Evaluate model performance under varying noise levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are repeated from the previous notebook, '02_Freddie_Freeloader.ipynb', and are grouped into one single step here for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set shape: x:(909, 365, 1), y:(909, 7, 1)\n",
      "validation set shape: x:(174, 365, 1), y:(174, 7, 1)\n",
      "Epoch 1/50\n",
      "129/129 [==============================] - 29s 228ms/step - loss: 0.0803 - mse: 0.0803 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 2/50\n",
      "129/129 [==============================] - 25s 193ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 3/50\n",
      "129/129 [==============================] - 25s 195ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 4/50\n",
      " 19/129 [===>..........................] - ETA: 19s - loss: 0.0114 - mse: 0.0114"
     ]
    }
   ],
   "source": [
    "# add random noise with zero mean and varying standard deviation as a separate feature to the input data \n",
    "standard_deviations = [0.01, 0.5, 1.0, 10.0]\n",
    "\n",
    "# initialize results dictionary\n",
    "res = {'model_type': [], 'std': [], 'val_mse': []\n",
    "       , 'mse': [], 'total_training_time': []}\n",
    "\n",
    "# model type \n",
    "model_types = ['RNN', 'LSTM']\n",
    "\n",
    "for model_type in model_types:\n",
    "    \n",
    "    for std in standard_deviations:\n",
    "        \n",
    "        # generate Gaussian noise\n",
    "        mean = 0.0\n",
    "        noise = [np.random.normal(loc=mean, scale=std, size=None) for x in range(len(idx))]\n",
    "\n",
    "        # initialize dataframe to store time series\n",
    "        df = pd.DataFrame(data={'observations': observations, 'noise': noise})\n",
    "        df.index = idx\n",
    "\n",
    "        # plot clean data and added noise\n",
    "        df.plot(alpha=0.5)\n",
    "        # save figure\n",
    "        fig_name = model_type + '_std_' + str(std) + '.jpg'\n",
    "        plt.savefig('../images/03_' + fig_name, dpi=500)\n",
    "\n",
    "        # load dataframe into object\n",
    "        mdq.df = df\n",
    "\n",
    "        # initialize dataset from dataframe \n",
    "        mdq.initialize_dataset()\n",
    "\n",
    "        # standardize data\n",
    "        mdq.standardize_data()\n",
    "\n",
    "        # generate train and validation data\n",
    "        mdq.generate_train_and_val_data(future_target_size=future_target_size, past_history_size=past_history_size)\n",
    "\n",
    "        # set number of steps per epoch\n",
    "        num_samples = mdq._num_samples\n",
    "        steps_per_epoch = int(num_samples/future_target_size)\n",
    "        validation_steps = int(steps_per_epoch/2)\n",
    "\n",
    "        # compile model\n",
    "        mdq.compile_model(units=units, num_layers=num_layers, model_type=model_type)\n",
    "\n",
    "        # fit model\n",
    "        mdq.fit_model(epochs=epochs, steps_per_epoch=steps_per_epoch\n",
    "                      ,validation_steps=validation_steps, model_type=model_type)\n",
    "\n",
    "        # get errors\n",
    "        history = mdq._histories[model_type]\n",
    "        val_mse = history.history['val_mse'][-1]\n",
    "        mse = history.history['mse'][-1]\n",
    "\n",
    "        # get total training time\n",
    "        total_training_time = sum(mdq._time_callbacks[model_type].times)\n",
    "\n",
    "        # append results to results dictionary\n",
    "        res['model_type'].append(model_type)\n",
    "        res['std'].append(std)\n",
    "        res['val_mse'].append(val_mse)\n",
    "        res['mse'].append(mse)\n",
    "        res['total_training_time'].append(total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part2.2_link'></a>\n",
    "### 2.2 Visualize and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dictionary to dataframe\n",
    "df_res = pd.DataFrame(res)\n",
    "\n",
    "# store dataframe as csv locally\n",
    "df_res.to_csv('../data/03_results_addedNoise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part3_link'></a>\n",
    "# 3. Distorted time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run RNN and LSTM model with noise added ontop of the clean data. Evaluate model performance for a range of variances of the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a range of dates on which the observations are made\n",
    "idx = pd.date_range(end='7/1/2020', periods=5*364, freq='d')\n",
    "\n",
    "# take a sine function as the observations\n",
    "num_periods = 10  # number of sine periods\n",
    "observations = [np.sin(2*np.pi*num_periods*x/len(idx)) for x in range(len(idx))]\n",
    "\n",
    "# initialize object\n",
    "mdq = Kind_of_Blue.Kind_of_Blue()\n",
    "\n",
    "# set target feature \n",
    "mdq._selected_features = ['observations']\n",
    "\n",
    "# set number of time points for 1/ future forecasting points and 2/ the past, historical time points\n",
    "future_target_size = int(365/52)\n",
    "past_history_size = int(1*365)\n",
    "\n",
    "# specify model configuration: this is chosen basen on the results from the previous notebook 02_Freddie_Freeloader.ipynb\n",
    "units = 256  # number of units in each neural network layer\n",
    "num_layers = 2  # total number of layers\n",
    "epochs = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part3.1_link'></a>\n",
    "### 3.1 Evaluate model performance under varying noise levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set shape: x:(909, 365, 1), y:(909, 7, 1)\n",
      "validation set shape: x:(174, 365, 1), y:(174, 7, 1)\n",
      "Epoch 1/50\n",
      "129/129 [==============================] - 29s 228ms/step - loss: 0.0803 - mse: 0.0803 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 2/50\n",
      "129/129 [==============================] - 25s 193ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 3/50\n",
      "129/129 [==============================] - 25s 195ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 4/50\n",
      " 19/129 [===>..........................] - ETA: 19s - loss: 0.0114 - mse: 0.0114"
     ]
    }
   ],
   "source": [
    "# add random noise with zero mean and varying standard deviation as a separate feature to the input data \n",
    "standard_deviations = [0.01, 0.5, 1.0, 10.0]\n",
    "\n",
    "# initialize results dictionary\n",
    "res = {'model_type': [], 'std': [], 'val_mse': []\n",
    "       , 'mse': [], 'total_training_time': []}\n",
    "\n",
    "# model type \n",
    "model_types = ['RNN', 'LSTM']\n",
    "\n",
    "for model_type in model_types:\n",
    "    \n",
    "    for std in standard_deviations:\n",
    "        \n",
    "        # generate Gaussian noise\n",
    "        mean = 0.0\n",
    "        noise = [np.random.normal(loc=mean, scale=std, size=None) for x in range(len(idx))]\n",
    "\n",
    "        # initialize dataframe to store time series\n",
    "        df = pd.DataFrame(data={'observations': observations, 'noise': noise})\n",
    "        df.index = idx\n",
    "\n",
    "        # plot clean data and added noise\n",
    "        df.plot(alpha=0.5)\n",
    "        # save figure\n",
    "        fig_name = model_type + '_std_' + str(std) + '.jpg'\n",
    "        plt.savefig('../images/03_' + fig_name, dpi=500)\n",
    "\n",
    "        # load dataframe into object\n",
    "        mdq.df = df\n",
    "\n",
    "        # initialize dataset from dataframe \n",
    "        mdq.initialize_dataset()\n",
    "\n",
    "        # standardize data\n",
    "        mdq.standardize_data()\n",
    "\n",
    "        # generate train and validation data\n",
    "        mdq.generate_train_and_val_data(future_target_size=future_target_size, past_history_size=past_history_size)\n",
    "\n",
    "        # set number of steps per epoch\n",
    "        num_samples = mdq._num_samples\n",
    "        steps_per_epoch = int(num_samples/future_target_size)\n",
    "        validation_steps = int(steps_per_epoch/2)\n",
    "\n",
    "        # compile model\n",
    "        mdq.compile_model(units=units, num_layers=num_layers, model_type=model_type)\n",
    "\n",
    "        # fit model\n",
    "        mdq.fit_model(epochs=epochs, steps_per_epoch=steps_per_epoch\n",
    "                      ,validation_steps=validation_steps, model_type=model_type)\n",
    "\n",
    "        # get errors\n",
    "        history = mdq._histories[model_type]\n",
    "        val_mse = history.history['val_mse'][-1]\n",
    "        mse = history.history['mse'][-1]\n",
    "\n",
    "        # get total training time\n",
    "        total_training_time = sum(mdq._time_callbacks[model_type].times)\n",
    "\n",
    "        # append results to results dictionary\n",
    "        res['model_type'].append(model_type)\n",
    "        res['std'].append(std)\n",
    "        res['val_mse'].append(val_mse)\n",
    "        res['mse'].append(mse)\n",
    "        res['total_training_time'].append(total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part3.2_link'></a>\n",
    "### 3.2 Visualize and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dictionary to dataframe\n",
    "df_res = pd.DataFrame(res)\n",
    "\n",
    "# store dataframe as csv locally\n",
    "df_res.to_csv('../data/03_results_distored.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
