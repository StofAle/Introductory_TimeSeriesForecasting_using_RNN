{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "#### In this notebook, the trial runs from the preceding notebook ('01_So_What.ipynb') are systematically extended to try to find an optimal configuration for the two models. The model parameters to be fine tuned are: number of epochs, number of layers and the number of units.\n",
    "\n",
    "#### The best performing model configuration will be used in the notebook '03_Blue_in_Green.ipynb' to analyze the impact of adding noise to the clean dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [1. Load modules](#Part1_link)\n",
    "* [2. Setup data](#Part2_link)\n",
    "<br >&nbsp;&nbsp;&nbsp;[2.1  Generate data, separate testing and validation set and standardize testing data](#Part2.1_link)\n",
    "* [3. Setup models and evaluate for various hyper-parameter choices](#Part3_link)\n",
    "<br >&nbsp;&nbsp;&nbsp;[3.1 Compile and fit LSTM and RNN model](#Part3.1_link)\n",
    "* [4. Visualize results](#Part4_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part1_link'></a>\n",
    "# 1. Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import Kind_of_Blue  # own class with a collection of methods used in this analysis\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part2_link'></a>\n",
    "# 2. Setup data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part2.1_link'></a>\n",
    "### 2.1 Generate data, separate testing and validation set and standardize testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are repeated from the previous notebook, '01_So_What.ipynb', and are grouped into one single step here for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations in time series: 1820\n",
      "train split ratio =  0.7\n",
      "loaded data set length: 1820\n",
      "mean: 0.0, std: 1.0\n",
      "debug3: check what buffer_size actually does! and why is data shape always (..., ..., 1) <- 1???\n",
      "training set shape: x:(909, 365, 1), y:(909, 7, 1)\n",
      "validation set shape: x:(174, 365, 1), y:(174, 7, 1)\n",
      "number of training samples: 909\n"
     ]
    }
   ],
   "source": [
    "# set a range of dates on which the observations are made\n",
    "idx = pd.date_range(end='7/1/2020', periods=5*364, freq='d')\n",
    "\n",
    "# take a sine function as the observations\n",
    "num_periods = 10  # number of sine periods\n",
    "observations = [np.sin(2*np.pi*num_periods*x/len(idx)) for x in range(len(idx))]\n",
    "print('number of observations in time series: {}'.format(len(observations)))\n",
    "\n",
    "# initialize dataframe to store time series\n",
    "df = pd.DataFrame(data=observations, columns=['observations'])\n",
    "df.index = idx\n",
    "\n",
    "# initialize object\n",
    "mdq = Kind_of_Blue.Kind_of_Blue()\n",
    "\n",
    "# load dataframe into object\n",
    "mdq._selected_features = ['observations']\n",
    "mdq.df = df\n",
    "\n",
    "# train-validation split ratio as class attribute set to 70%\n",
    "print('train split ratio = ', mdq.TRAIN_SPLIT_RATIO)\n",
    "\n",
    "# initialize dataset from dataframe \n",
    "mdq.initialize_dataset()\n",
    "print('loaded data set length: {}'.format(len(mdq._dataset)))\n",
    "\n",
    "# standardize data\n",
    "mdq.standardize_data()\n",
    "\n",
    "# check that mean equals zero and the standard deviation is one\n",
    "print('mean: {}, std: {}'.format(round(np.mean(mdq._dataset), 2), round(np.std(mdq._dataset), 2)))\n",
    "\n",
    "# set number of time points for 1/ future forecasting points and 2/ the past, historical time points\n",
    "future_target_size = int(365/52)\n",
    "past_history_size = int(1*365)\n",
    "\n",
    "# set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# generate train and validation data\n",
    "mdq.generate_train_and_val_data(future_target_size=future_target_size, past_history_size=past_history_size\n",
    "                                , batch_size=batch_size)\n",
    "\n",
    "print('number of training samples: {}'.format(mdq._num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part3_link'></a>\n",
    "# 3. Setup models and evaluate for various hyper-parameter choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part3.1_link'></a>\n",
    "### 3.1 Compile and fit LSTM and RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator for configurations to be iterated over\n",
    "\n",
    "def config_generator():\n",
    "    \n",
    "    unit_choices = [256, 512]  # number of units in each neural network layer\n",
    "    layer_choices = [2]  # total number of layers\n",
    "    epoch_choices = [50, 100]  # number of epochs the model is trained on\n",
    "    \n",
    "    for units in unit_choices:\n",
    "        for num_layers in layer_choices:\n",
    "            for epochs in epoch_choices:\n",
    "                yield units, num_layers, epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently running LSTM model\n",
      "debugLSTM: should an LSTM layer or a Dense layer be added?\n",
      "Epoch 1/10\n",
      "28/28 [==============================] - 9s 319ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 7s 265ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 7s 247ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 7s 248ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 7s 252ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 7s 255ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 8s 272ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 7s 253ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 7s 248ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 7s 240ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "currently running RNN model\n",
      "Epoch 1/10\n",
      "28/28 [==============================] - 2s 74ms/step - loss: 0.4848 - mse: 0.4848 - val_loss: 0.0823 - val_mse: 0.0823\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 2s 67ms/step - loss: 0.0970 - mse: 0.0970 - val_loss: 0.0221 - val_mse: 0.0221\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 2s 68ms/step - loss: 0.0687 - mse: 0.0687 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 2s 59ms/step - loss: 0.0577 - mse: 0.0577 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 2s 57ms/step - loss: 0.0515 - mse: 0.0515 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 2s 61ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 2s 62ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 2s 59ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 2s 60ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 2s 60ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 8.8995e-04 - val_mse: 8.8995e-04\n",
      "currently running LSTM model\n",
      "debugLSTM: should an LSTM layer or a Dense layer be added?\n",
      "Epoch 1/30\n",
      "28/28 [==============================] - 8s 300ms/step - loss: 0.5238 - mse: 0.5238 - val_loss: 0.1486 - val_mse: 0.1486\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 7s 254ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 7s 258ms/step - loss: 0.0545 - mse: 0.0545 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 8s 279ms/step - loss: 0.0508 - mse: 0.0508 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 8s 291ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 7s 260ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 8.4041e-04 - val_mse: 8.4041e-04\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 8s 276ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 9s 324ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 9s 311ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 11s 383ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 9s 337ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 12/30\n",
      "28/28 [==============================] - 8s 298ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 8s 301ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 14/30\n",
      "28/28 [==============================] - 9s 304ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 7s 266ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 7s 264ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 8s 270ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 7s 263ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 7s 266ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 8s 269ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 7s 267ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 8s 270ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 8s 271ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 8s 269ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 8s 271ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 8s 269ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 8s 273ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 8s 270ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 8s 280ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 8s 272ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 9.9449e-04 - val_mse: 9.9449e-04\n",
      "currently running RNN model\n",
      "Epoch 1/30\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 0.4452 - mse: 0.4452 - val_loss: 0.0612 - val_mse: 0.0612\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 2s 68ms/step - loss: 0.0897 - mse: 0.0897 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 2s 66ms/step - loss: 0.0617 - mse: 0.0617 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 2s 68ms/step - loss: 0.0527 - mse: 0.0527 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 2s 66ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 2s 65ms/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 2s 71ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 2s 66ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 2s 65ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 2s 62ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 2s 65ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0117 - val_mse: 0.0117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "28/28 [==============================] - 2s 66ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 2s 65ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 14/30\n",
      "28/28 [==============================] - 2s 66ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 2s 65ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 6.8070e-04 - val_mse: 6.8070e-04\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 2s 63ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 2s 66ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 2s 66ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 8.1999e-04 - val_mse: 8.1999e-04\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 2s 66ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 2s 67ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 6.4679e-04 - val_mse: 6.4679e-04\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 2s 66ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 2s 71ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 2s 65ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 2s 62ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 3.8508e-04 - val_mse: 3.8508e-04\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 2s 65ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 2s 67ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 7.6353e-04 - val_mse: 7.6353e-04\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 2s 67ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 8.7790e-04 - val_mse: 8.7790e-04\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 2s 66ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 2s 64ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 4.6691e-04 - val_mse: 4.6691e-04\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 2s 65ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "currently running LSTM model\n",
      "debugLSTM: should an LSTM layer or a Dense layer be added?\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 11s 381ms/step - loss: 0.5672 - mse: 0.5672 - val_loss: 0.1786 - val_mse: 0.1786\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 10s 348ms/step - loss: 0.1270 - mse: 0.1270 - val_loss: 0.0291 - val_mse: 0.0291\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 8s 278ms/step - loss: 0.0673 - mse: 0.0673 - val_loss: 0.0151 - val_mse: 0.0151\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 8s 274ms/step - loss: 0.0545 - mse: 0.0545 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 8s 272ms/step - loss: 0.0456 - mse: 0.0456 - val_loss: 0.0201 - val_mse: 0.0201\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 8s 273ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 8s 277ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 8s 270ms/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 8s 280ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 9s 318ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 8s 281ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 8s 274ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 8s 276ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 8s 275ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 8s 274ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 8s 275ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 8s 280ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 8.0167e-04 - val_mse: 8.0167e-04\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 8s 288ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 8s 292ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 7.6932e-04 - val_mse: 7.6932e-04\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 8s 278ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 8s 279ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 8s 278ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 8s 277ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 6.3857e-04 - val_mse: 6.3857e-04\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 8s 277ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 8s 280ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 8s 281ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 8s 278ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 8s 281ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 8s 275ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 6.8729e-04 - val_mse: 6.8729e-04\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 8s 282ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 9s 306ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 3.3497e-04 - val_mse: 3.3497e-04\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 8s 277ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 8s 280ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 6.0094e-04 - val_mse: 6.0094e-04\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 8s 281ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 8s 281ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 8s 277ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 8s 285ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 8s 278ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 8s 276ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 8s 279ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 8s 283ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 7.3255e-04 - val_mse: 7.3255e-04\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 8s 279ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 8s 284ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 8s 289ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 3.2073e-04 - val_mse: 3.2073e-04\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 8s 283ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 7.4994e-04 - val_mse: 7.4994e-04\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 8s 277ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 5.2683e-04 - val_mse: 5.2683e-04\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 8s 277ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 8s 280ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 8s 276ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 8s 277ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "currently running RNN model\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 2s 79ms/step - loss: 0.3485 - mse: 0.3485 - val_loss: 0.0284 - val_mse: 0.0284\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 2s 74ms/step - loss: 0.0765 - mse: 0.0765 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 2s 68ms/step - loss: 0.0601 - mse: 0.0601 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 2s 71ms/step - loss: 0.0444 - mse: 0.0444 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 3s 92ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 2s 78ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 2s 71ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 2s 73ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 2s 70ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 2s 70ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 2s 71ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 8.7824e-04 - val_mse: 8.7824e-04\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 2s 69ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 2s 70ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 7.6579e-04 - val_mse: 7.6579e-04\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 2s 67ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 3s 107ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 51s 2s/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 4s 144ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 4s 148ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 4s 150ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 7.5108e-04 - val_mse: 7.5108e-04\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 2s 69ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 2s 70ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 2s 65ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 2s 61ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 2s 57ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 2s 56ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 2s 56ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 4.3156e-04 - val_mse: 4.3156e-04\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 2s 68ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 2s 68ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 6.4648e-04 - val_mse: 6.4648e-04\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 2s 64ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 2s 69ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 7.9459e-04 - val_mse: 7.9459e-04\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 2s 63ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 5.0835e-04 - val_mse: 5.0835e-04\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 2s 57ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 2s 58ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 2s 57ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 2s 59ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 2s 56ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 2s 62ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 2s 59ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 2s 58ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 2s 59ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 531s 19s/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 2s 69ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 6.7709e-04 - val_mse: 6.7709e-04\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 2s 58ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 3.9171e-04 - val_mse: 3.9171e-04\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 2s 59ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 2s 63ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 2s 59ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 11s 401ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 7.2659e-04 - val_mse: 7.2659e-04\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 2s 61ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 8.2375e-04 - val_mse: 8.2375e-04\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 2s 61ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 6.3748e-04 - val_mse: 6.3748e-04\n",
      "currently running LSTM model\n",
      "debugLSTM: should an LSTM layer or a Dense layer be added?\n",
      "Epoch 1/10\n",
      "28/28 [==============================] - 15s 527ms/step - loss: 0.4118 - mse: 0.4118 - val_loss: 0.0799 - val_mse: 0.0799\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 216s 8s/step - loss: 0.0574 - mse: 0.0574 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 13s 479ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 28s 1s/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 262s 9s/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 22s 786ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 13s 482ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 19s 664ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 14s 508ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 13s 481ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "currently running RNN model\n",
      "Epoch 1/10\n",
      "28/28 [==============================] - 2s 80ms/step - loss: 0.1850 - mse: 0.1850 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 2s 75ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 2s 73ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 2s 73ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 2s 73ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 2s 72ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 2s 73ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 2s 70ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 2s 71ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0055 - val_mse: 0.0055\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 2s 70ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "currently running LSTM model\n",
      "debugLSTM: should an LSTM layer or a Dense layer be added?\n",
      "Epoch 1/30\n",
      "28/28 [==============================] - 14s 488ms/step - loss: 0.4056 - mse: 0.4056 - val_loss: 0.0656 - val_mse: 0.0656\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 14s 517ms/step - loss: 0.0566 - mse: 0.0566 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 14s 487ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 14s 511ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 18s 654ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 14s 493ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 13s 475ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0057 - val_mse: 0.0057\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 15s 543ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 18s 644ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 14s 506ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 14s 506ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 12/30\n",
      "28/28 [==============================] - 14s 506ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0200 - val_mse: 0.0200\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 14s 504ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 14/30\n",
      "28/28 [==============================] - 20s 722ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 14s 484ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 7.0985e-04 - val_mse: 7.0985e-04\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 14s 491ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 6.2773e-04 - val_mse: 6.2773e-04\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 14s 508ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 14s 510ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 14s 491ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 8.5050e-04 - val_mse: 8.5050e-04\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 17s 609ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 14s 487ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 13s 474ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 13s 481ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 9.9548e-04 - val_mse: 9.9548e-04\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 17s 593ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 9.5080e-04 - val_mse: 9.5080e-04\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 15s 547ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 9.2625e-04 - val_mse: 9.2625e-04\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 14s 512ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 16s 582ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 9.9668e-04 - val_mse: 9.9668e-04\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 20s 700ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 6.5164e-04 - val_mse: 6.5164e-04\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 16s 557ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 9.5100e-04 - val_mse: 9.5100e-04\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 21s 740ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "currently running RNN model\n",
      "Epoch 1/30\n",
      "28/28 [==============================] - 3s 90ms/step - loss: 0.2093 - mse: 0.2093 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 3s 95ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 2s 80ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 2s 74ms/step - loss: 0.0623 - mse: 0.0623 - val_loss: 0.3690 - val_mse: 0.3690\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 0.2136 - mse: 0.2136 - val_loss: 0.0473 - val_mse: 0.0473\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 2s 85ms/step - loss: 0.2893 - mse: 0.2893 - val_loss: 0.1693 - val_mse: 0.1693\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 2s 85ms/step - loss: 0.0696 - mse: 0.0696 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 2s 68ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 2s 63ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 2s 67ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 2s 63ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 6.9618e-04 - val_mse: 6.9618e-04\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 2s 69ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 2s 77ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 14/30\n",
      "28/28 [==============================] - 2s 80ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 2s 69ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 2s 70ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 9.4574e-04 - val_mse: 9.4574e-04\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 2s 85ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 4s 129ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 7.7980e-04 - val_mse: 7.7980e-04\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 3s 108ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 3s 116ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 2s 89ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 2s 82ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 2s 80ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 2s 77ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 9.8067e-04 - val_mse: 9.8067e-04\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 3s 104ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 3s 100ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 6.1543e-04 - val_mse: 6.1543e-04\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 3s 102ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 7.3697e-04 - val_mse: 7.3697e-04\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 3s 114ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 3s 112ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "currently running LSTM model\n",
      "debugLSTM: should an LSTM layer or a Dense layer be added?\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 28s 1s/step - loss: 0.4144 - mse: 0.4144 - val_loss: 0.0927 - val_mse: 0.0927\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 30s 1s/step - loss: 0.0583 - mse: 0.0583 - val_loss: 0.0438 - val_mse: 0.0438\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 27s 964ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 23s 815ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 20s 702ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 9.5342e-04 - val_mse: 9.5342e-04\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 22s 770ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 26s 933ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 6.5246e-04 - val_mse: 6.5246e-04\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 23s 812ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 20s 699ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 9.1887e-04 - val_mse: 9.1887e-04\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 21s 735ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 21s 754ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 8.4519e-04 - val_mse: 8.4519e-04\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 20s 717ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 21s 766ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 21s 747ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 21s 765ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 21s 764ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 19s 664ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 24s 859ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 30s 1s/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 28s 1s/step - loss: 0.0128 - mse: 0.0128 - val_loss: 2.6489e-04 - val_mse: 2.6489e-04\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 28s 1s/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 21s 736ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 27s 977ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 21s 742ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 6.3511e-04 - val_mse: 6.3511e-04\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 20s 718ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 20s 703ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 19s 694ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 20s 713ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 22s 802ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 24s 843ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 20s 719ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 18s 644ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 7.2970e-04 - val_mse: 7.2970e-04\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 20s 726ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 8.8913e-04 - val_mse: 8.8913e-04\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 28s 1s/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 18s 653ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 19s 683ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 3.8905e-04 - val_mse: 3.8905e-04\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 19s 682ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 18s 649ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 18s 650ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 7.4129e-04 - val_mse: 7.4129e-04\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 19s 690ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 3.5679e-04 - val_mse: 3.5679e-04\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 20s 712ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 22s 776ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 3.7343e-04 - val_mse: 3.7343e-04\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 21s 736ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 3.9881e-04 - val_mse: 3.9881e-04\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 5.4644e-04 - val_mse: 5.4644e-04\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 20s 713ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 24s 853ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "currently running RNN model\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 3s 116ms/step - loss: 0.2899 - mse: 0.2899 - val_loss: 0.0845 - val_mse: 0.0845\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 4s 130ms/step - loss: 0.0906 - mse: 0.0906 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 3s 117ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 3s 109ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 3s 98ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 3s 98ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 3s 106ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 3s 100ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 3s 105ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 3s 107ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 3s 115ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 3s 104ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 3s 107ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 3s 105ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0048 - val_mse: 0.0048\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 3s 108ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 3s 105ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 3s 102ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 3s 105ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 3s 110ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 3s 107ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 3s 97ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 3s 100ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 3s 101ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 3s 102ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 3s 107ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 3s 104ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 3s 98ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 3s 102ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 3s 109ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 6.1155e-04 - val_mse: 6.1155e-04\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 3s 99ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 9.8358e-04 - val_mse: 9.8358e-04\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 3s 101ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 3s 104ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 5.2787e-04 - val_mse: 5.2787e-04\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 3s 103ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 8.8081e-04 - val_mse: 8.8081e-04\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 3s 107ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 3s 108ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.2681 - val_mse: 0.2681\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 3s 103ms/step - loss: 1.0532 - mse: 1.0532 - val_loss: 0.4468 - val_mse: 0.4468\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 3s 102ms/step - loss: 0.3599 - mse: 0.3599 - val_loss: 0.1007 - val_mse: 0.1007\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 3s 98ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.0278 - val_mse: 0.0278\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 3s 106ms/step - loss: 0.0643 - mse: 0.0643 - val_loss: 0.0262 - val_mse: 0.0262\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 3s 100ms/step - loss: 0.0539 - mse: 0.0539 - val_loss: 0.0267 - val_mse: 0.0267\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 3s 97ms/step - loss: 0.0505 - mse: 0.0505 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 3s 99ms/step - loss: 0.0493 - mse: 0.0493 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 3s 103ms/step - loss: 0.0577 - mse: 0.0577 - val_loss: 0.0280 - val_mse: 0.0280\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 3s 104ms/step - loss: 0.0550 - mse: 0.0550 - val_loss: 0.0267 - val_mse: 0.0267\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 3s 104ms/step - loss: 0.0461 - mse: 0.0461 - val_loss: 0.0240 - val_mse: 0.0240\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 3s 101ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.0279 - val_mse: 0.0279\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 3s 104ms/step - loss: 0.0447 - mse: 0.0447 - val_loss: 0.0243 - val_mse: 0.0243\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 3s 108ms/step - loss: 0.0460 - mse: 0.0460 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 3s 101ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.0243 - val_mse: 0.0243\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 3s 105ms/step - loss: 0.0439 - mse: 0.0439 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "currently running LSTM model\n",
      "debugLSTM: should an LSTM layer or a Dense layer be added?\n",
      "Epoch 1/10\n",
      "28/28 [==============================] - 56s 2s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 58s 2s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 51s 2s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 52s 2s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 52s 2s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 50s 2s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 61s 2s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 105s 4s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 55s 2s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 54s 2s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "currently running RNN model\n",
      "Epoch 1/10\n",
      "28/28 [==============================] - 9s 336ms/step - loss: 0.4263 - mse: 0.4263 - val_loss: 0.0909 - val_mse: 0.0909\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 7s 263ms/step - loss: 0.2128 - mse: 0.2128 - val_loss: 0.0811 - val_mse: 0.0811\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 7s 239ms/step - loss: 0.2598 - mse: 0.2598 - val_loss: 0.3058 - val_mse: 0.3058\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 6s 231ms/step - loss: 0.2359 - mse: 0.2359 - val_loss: 0.0594 - val_mse: 0.0594\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 7s 248ms/step - loss: 0.2081 - mse: 0.2081 - val_loss: 0.1200 - val_mse: 0.1200\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 7s 242ms/step - loss: 0.2778 - mse: 0.2778 - val_loss: 0.1579 - val_mse: 0.1579\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 7s 246ms/step - loss: 0.2116 - mse: 0.2116 - val_loss: 0.1677 - val_mse: 0.1677\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 7s 253ms/step - loss: 0.1571 - mse: 0.1571 - val_loss: 0.0488 - val_mse: 0.0488\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 8s 299ms/step - loss: 0.1118 - mse: 0.1118 - val_loss: 0.0765 - val_mse: 0.0765\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 7s 262ms/step - loss: 0.0948 - mse: 0.0948 - val_loss: 0.0431 - val_mse: 0.0431\n",
      "currently running LSTM model\n",
      "debugLSTM: should an LSTM layer or a Dense layer be added?\n",
      "Epoch 1/30\n",
      "28/28 [==============================] - 67s 2s/step - loss: 0.3136 - mse: 0.3136 - val_loss: 0.0522 - val_mse: 0.0522\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 58s 2s/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 74s 3s/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0196 - val_mse: 0.0196\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 51s 2s/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 58s 2s/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 67s 2s/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 61s 2s/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 65s 2s/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 68s 2s/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 62s 2s/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 60s 2s/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 12/30\n",
      "28/28 [==============================] - 58s 2s/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 56s 2s/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 14/30\n",
      "28/28 [==============================] - 57s 2s/step - loss: 0.0084 - mse: 0.0084 - val_loss: 8.1177e-04 - val_mse: 8.1177e-04\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 66s 2s/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 58s 2s/step - loss: 0.0086 - mse: 0.0086 - val_loss: 7.9283e-04 - val_mse: 7.9283e-04\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 53s 2s/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0065 - mse: 0.0065 - val_loss: 4.1598e-04 - val_mse: 4.1598e-04\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0068 - mse: 0.0068 - val_loss: 2.1238e-04 - val_mse: 2.1238e-04\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 41s 1s/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "currently running RNN model\n",
      "Epoch 1/30\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 0.3253 - mse: 0.3253 - val_loss: 0.1372 - val_mse: 0.1372\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 5s 175ms/step - loss: 0.1466 - mse: 0.1466 - val_loss: 0.0478 - val_mse: 0.0478\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.0539 - mse: 0.0539 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 4s 148ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 4s 144ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 4s 143ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 4s 142ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 4s 143ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 4s 140ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 4s 140ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 4s 142ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 12/30\n",
      "28/28 [==============================] - 4s 145ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 4s 143ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 4s 141ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 4s 145ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 4s 142ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 4s 144ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 4s 144ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 4s 140ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 4s 156ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 8.5932e-04 - val_mse: 8.5932e-04\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 4s 146ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 4s 145ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 4s 143ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 4s 139ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 4s 142ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 4s 144ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 4s 140ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 4s 140ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 4s 140ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 4s 143ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "currently running LSTM model\n",
      "debugLSTM: should an LSTM layer or a Dense layer be added?\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 34s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 35s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 47s 2s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 848s 30s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 37s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 33s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 33s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 32s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 32s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 33s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 38s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 34s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 32s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 34s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 33s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 32s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 33s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 39s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 37s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 34s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 32s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 32s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 37s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 32s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 31s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 34s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 34s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 41s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 40s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 32s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 32s 1s/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "currently running RNN model\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 4s 143ms/step - loss: 0.1667 - mse: 0.1667 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 4s 139ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0589 - val_mse: 0.0589\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 4s 140ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 4s 140ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 4s 139ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 4s 142ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 9.2640e-04 - val_mse: 9.2640e-04\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 4s 149ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 5s 177ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 4s 149ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 3.1269 - val_mse: 3.1269\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 2.3406 - mse: 2.3406 - val_loss: 0.5922 - val_mse: 0.5922\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 1.4680 - mse: 1.4680 - val_loss: 0.2512 - val_mse: 0.2512\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.3612 - mse: 0.3612 - val_loss: 0.2380 - val_mse: 0.2380\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 4s 141ms/step - loss: 0.2837 - mse: 0.2837 - val_loss: 0.2278 - val_mse: 0.2278\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 4s 134ms/step - loss: 0.2640 - mse: 0.2640 - val_loss: 0.2259 - val_mse: 0.2259\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.2617 - mse: 0.2617 - val_loss: 0.2279 - val_mse: 0.2279\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.2565 - mse: 0.2565 - val_loss: 0.2358 - val_mse: 0.2358\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.2509 - mse: 0.2509 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 4s 149ms/step - loss: 0.2510 - mse: 0.2510 - val_loss: 0.2149 - val_mse: 0.2149\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 4s 142ms/step - loss: 0.2581 - mse: 0.2581 - val_loss: 0.2101 - val_mse: 0.2101\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 4s 142ms/step - loss: 0.2367 - mse: 0.2367 - val_loss: 0.1997 - val_mse: 0.1997\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 4s 152ms/step - loss: 0.2263 - mse: 0.2263 - val_loss: 0.1944 - val_mse: 0.1944\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 4s 143ms/step - loss: 0.2274 - mse: 0.2274 - val_loss: 0.2003 - val_mse: 0.2003\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.2080 - mse: 0.2080 - val_loss: 0.1755 - val_mse: 0.1755\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.1921 - mse: 0.1921 - val_loss: 0.1582 - val_mse: 0.1582\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 4s 156ms/step - loss: 0.1696 - mse: 0.1696 - val_loss: 0.1380 - val_mse: 0.1380\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 4s 154ms/step - loss: 0.1552 - mse: 0.1552 - val_loss: 0.1122 - val_mse: 0.1122\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 4s 148ms/step - loss: 0.1258 - mse: 0.1258 - val_loss: 0.0965 - val_mse: 0.0965\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 4s 150ms/step - loss: 0.1033 - mse: 0.1033 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 4s 144ms/step - loss: 0.0941 - mse: 0.0941 - val_loss: 0.0671 - val_mse: 0.0671\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 4s 137ms/step - loss: 0.0931 - mse: 0.0931 - val_loss: 0.0664 - val_mse: 0.0664\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 4s 146ms/step - loss: 0.0804 - mse: 0.0804 - val_loss: 0.0787 - val_mse: 0.0787\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 4s 144ms/step - loss: 0.0793 - mse: 0.0793 - val_loss: 0.0629 - val_mse: 0.0629\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 4s 140ms/step - loss: 0.0820 - mse: 0.0820 - val_loss: 0.0729 - val_mse: 0.0729\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 4s 146ms/step - loss: 0.0824 - mse: 0.0824 - val_loss: 0.0598 - val_mse: 0.0598\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 4s 148ms/step - loss: 0.0772 - mse: 0.0772 - val_loss: 0.0761 - val_mse: 0.0761\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 4s 141ms/step - loss: 0.0805 - mse: 0.0805 - val_loss: 0.0919 - val_mse: 0.0919\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 4s 141ms/step - loss: 0.0766 - mse: 0.0766 - val_loss: 0.0585 - val_mse: 0.0585\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.0858 - mse: 0.0858 - val_loss: 0.0849 - val_mse: 0.0849\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 5s 175ms/step - loss: 0.0804 - mse: 0.0804 - val_loss: 0.0585 - val_mse: 0.0585\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.0701 - mse: 0.0701 - val_loss: 0.0572 - val_mse: 0.0572\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 4s 141ms/step - loss: 0.0743 - mse: 0.0743 - val_loss: 0.0539 - val_mse: 0.0539\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 4s 149ms/step - loss: 0.0658 - mse: 0.0658 - val_loss: 0.0482 - val_mse: 0.0482\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 4s 150ms/step - loss: 0.0604 - mse: 0.0604 - val_loss: 0.0454 - val_mse: 0.0454\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 4s 141ms/step - loss: 0.0653 - mse: 0.0653 - val_loss: 0.0457 - val_mse: 0.0457\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 4s 148ms/step - loss: 0.0618 - mse: 0.0618 - val_loss: 0.0474 - val_mse: 0.0474\n"
     ]
    }
   ],
   "source": [
    "# iterations over the model parameter configurations are done for both LSTM as well as RNN model\n",
    "model_types = ['LSTM', 'RNN']\n",
    "\n",
    "# set number of steps per epoch\n",
    "num_samples = mdq._num_samples\n",
    "steps_per_epoch = int(num_samples/batch_size)\n",
    "validation_steps = int(steps_per_epoch/2)\n",
    "\n",
    "# initialize results dictionary\n",
    "res = {'model_type': [], 'epochs': [], 'num_layers': [], 'units': [], 'val_mse': []\n",
    "       , 'mse': [], 'total_training_time': []}\n",
    "\n",
    "for units, num_layers, epochs in config_generator():\n",
    "    for model_type in model_types:\n",
    "\n",
    "        print('currently running {} model'.format(model_type))\n",
    "\n",
    "        # compile model\n",
    "        mdq.compile_model(units=units, num_layers=num_layers, model_type=model_type)\n",
    "\n",
    "        # fit model\n",
    "        mdq.fit_model(epochs=epochs, steps_per_epoch=steps_per_epoch\n",
    "                      ,validation_steps=validation_steps, model_type=model_type)\n",
    "        \n",
    "        # get errors\n",
    "        history = mdq._histories[model_type]\n",
    "        val_mse = history.history['val_mse'][-1]\n",
    "        mse = history.history['mse'][-1]\n",
    "        \n",
    "        # get total training time\n",
    "        total_training_time = sum(mdq._time_callbacks[model_type].times)\n",
    "        \n",
    "        # append results to results dictionary\n",
    "        res['model_type'].append(model_type)\n",
    "        res['epochs'].append(epochs)\n",
    "        res['num_layers'].append(num_layers)\n",
    "        res['units'].append(units)\n",
    "        res['val_mse'].append(val_mse)\n",
    "        res['mse'].append(mse)\n",
    "        res['total_training_time'].append(total_training_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part4_link'></a>\n",
    "# 4. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dictionary to dataframe\n",
    "df_res = pd.DataFrame(res)\n",
    "\n",
    "# store dataframe as csv locally\n",
    "# df_res.to_csv('../data/02_results_run4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize results; use bubble plots to indicate magnitude of mean-square error for specific configuration comparing\n",
    "# RNN to LSTM results\n",
    "\n",
    "x_label = 'epochs'\n",
    "y_label = 'units'\n",
    "z_label = 'mse'\n",
    "# condition_label = 'num_layers'\n",
    "# condition_vals = list(set(df_res[condition_label]))\n",
    "\n",
    "condition_LSTM = (df_res['model_type']=='LSTM')\n",
    "condition_RNN = (df_res['model_type']=='RNN')\n",
    "\n",
    "for model_type in ['LSTM', 'RNN']:\n",
    "    condition_1 = (df_res['model_type']== model_type)\n",
    "    \n",
    "    x = df_res[condition_1][x_label].values\n",
    "    y = df_res[condition_1][y_label].values\n",
    "    \n",
    "    z = df_res[condition_1][z_label].values\n",
    "    plt.scatter(x, y, s=z*10000, alpha=0.6, c=\"red\", linewidth=0.0)\n",
    "        \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title('mean-square training error: {} model'.format(model_type))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>epochs</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>units</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>mse</th>\n",
       "      <th>total_training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.032671</td>\n",
       "      <td>18.739033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RNN</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.018335</td>\n",
       "      <td>58.067086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RNN</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.013823</td>\n",
       "      <td>691.488427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.017373</td>\n",
       "      <td>21.719517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RNN</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.012532</td>\n",
       "      <td>75.930581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RNN</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.029862</td>\n",
       "      <td>0.043877</td>\n",
       "      <td>152.320554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.043066</td>\n",
       "      <td>0.094807</td>\n",
       "      <td>76.747302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RNN</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.008963</td>\n",
       "      <td>126.177479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RNN</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.047449</td>\n",
       "      <td>0.061842</td>\n",
       "      <td>219.223186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_type  epochs  num_layers  units   val_mse       mse  \\\n",
       "1         RNN      10           2     64  0.000890  0.032671   \n",
       "3         RNN      30           2     64  0.003220  0.018335   \n",
       "5         RNN      50           2     64  0.000637  0.013823   \n",
       "7         RNN      10           2    128  0.001506  0.017373   \n",
       "9         RNN      30           2    128  0.002504  0.012532   \n",
       "11        RNN      50           2    128  0.029862  0.043877   \n",
       "13        RNN      10           2    256  0.043066  0.094807   \n",
       "15        RNN      30           2    256  0.001219  0.008963   \n",
       "17        RNN      50           2    256  0.047449  0.061842   \n",
       "\n",
       "    total_training_time  \n",
       "1             18.739033  \n",
       "3             58.067086  \n",
       "5            691.488427  \n",
       "7             21.719517  \n",
       "9             75.930581  \n",
       "11           152.320554  \n",
       "13            76.747302  \n",
       "15           126.177479  \n",
       "17           219.223186  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res[df_res['model_type']=='RNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
